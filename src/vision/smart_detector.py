#!/usr/bin/env python3
"""
–£–º–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –∫–∞–ø—á.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç CLIP –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —è—á–µ–µ–∫ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –æ–ø–∏—Å–∞–Ω–∏—è–º.
"""

import torch
from PIL import Image
import numpy as np
from pathlib import Path


class SmartDetector:
    """–£–º–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º CLIP"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.processor = None
        self.model_loaded = False
        
        print(f"ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–º–Ω–æ–≥–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –Ω–∞ {self.device}")
        
    def load_clip_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç CLIP –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        try:
            print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ CLIP –º–æ–¥–µ–ª–∏...")
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã CLIP
            try:
                # OpenCLIP (–ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ)
                import open_clip
                
                model_name = "ViT-B-32"
                pretrained = "openai"
                
                self.model, _, self.processor = open_clip.create_model_and_transforms(
                    model_name, pretrained=pretrained, device=self.device
                )
                self.tokenizer = open_clip.get_tokenizer(model_name)
                
                print(f"‚úÖ OpenCLIP –∑–∞–≥—Ä—É–∂–µ–Ω: {model_name}")
                self.model_type = "openclip"
                
            except ImportError:
                # Fallback –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É CLIP —á–µ—Ä–µ–∑ transformers
                from transformers import CLIPProcessor, CLIPModel
                
                model_name = "openai/clip-vit-base-patch32"
                self.model = CLIPModel.from_pretrained(model_name).to(self.device)
                self.processor = CLIPProcessor.from_pretrained(model_name)
                
                print(f"‚úÖ Transformers CLIP –∑–∞–≥—Ä—É–∂–µ–Ω: {model_name}")
                self.model_type = "transformers"
            
            self.model_loaded = True
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CLIP: {e}")
            return False
    
    def analyze_cell_with_clip(self, cell_image, instruction):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —è—á–µ–π–∫—É —Å –ø–æ–º–æ—â—å—é CLIP"""
        if not self.model_loaded:
            print("‚ùå CLIP –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            return 0.0, "–º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
        
        try:
            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            possible_objects = self._extract_target_objects(instruction)
            
            if not possible_objects:
                return 0.0, "–Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ü–µ–ª–µ–≤—ã–µ –æ–±—ä–µ–∫—Ç—ã"
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –≤–æ–∑–º–æ–∂–Ω—ã–π –æ–±—ä–µ–∫—Ç
            best_score = 0.0
            best_object = ""
            
            for obj in possible_objects:
                score = self._calculate_clip_score(cell_image, obj)
                if score > best_score:
                    best_score = score
                    best_object = obj
            
            return best_score, best_object
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ CLIP –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return 0.0, f"–æ—à–∏–±–∫–∞: {e}"
    
    def _extract_target_objects(self, instruction):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–ª–µ–≤—ã–µ –æ–±—ä–µ–∫—Ç—ã –∏–∑ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"""
        instruction_lower = instruction.lower()
        
        # –°–ª–æ–≤–∞—Ä—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤
        object_mapping = {
            # –ú–µ–±–µ–ª—å –¥–ª—è —Å–∏–¥–µ–Ω–∏—è
            "—Å–∏–¥–µ—Ç—å": ["chair", "sofa", "bench", "stool", "armchair"],
            "—Å—Ç—É–ª": ["chair", "stool", "seat"],
            "–∫—Ä–µ—Å–ª–æ": ["armchair", "chair", "recliner"],
            
            # –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç
            "–∞–≤—Ç–æ–º–æ–±–∏–ª—å": ["car", "vehicle", "automobile", "truck"],
            "–º–∞—à–∏–Ω": ["car", "vehicle", "automobile"],
            "—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç": ["vehicle", "car", "truck", "bus"],
            
            # –ü—Ä–∏—Ä–æ–¥–∞ –∏ –ª–µ—Å
            "–ª–µ—Å": ["forest", "trees", "woods"],
            "–¥–µ—Ä–µ–≤": ["tree", "trees", "forest"],
            "—Ä–∞—Å—Ç–µ–Ω–∏": ["plant", "vegetation", "flora"],
            
            # –í–æ–¥–∞ –∏ –æ–∫–µ–∞–Ω
            "–æ–∫–µ–∞–Ω": ["ocean", "sea", "water", "waves"],
            "–º–æ—Ä–µ": ["sea", "ocean", "water"],
            "–≤–æ–¥–∞": ["water", "ocean", "sea", "river"],
            
            # –ü—Ä–µ–¥–º–µ—Ç—ã –¥–ª—è –ø–æ–¥–≤–µ—à–∏–≤–∞–Ω–∏—è
            "–ø–æ–≤–µ—Å–∏—Ç—å": ["painting", "picture", "frame", "mirror", "lamp", "clock"],
            "–∫–∞—Ä—Ç–∏–Ω": ["painting", "picture", "artwork", "frame"],
            "–∑–µ—Ä–∫–∞–ª": ["mirror", "reflection"],
            
            # –ï–¥–∞
            "–µ–¥–∞": ["food", "meal", "dish"],
            "—Ñ—Ä—É–∫—Ç": ["fruit", "apple", "banana", "orange"],
            
            # –ñ–∏–≤–æ—Ç–Ω—ã–µ
            "–∂–∏–≤–æ—Ç–Ω": ["animal", "pet", "dog", "cat", "bird"],
            "—Å–æ–±–∞–∫": ["dog", "puppy"],
            "–∫–æ—Ç": ["cat", "kitten"],
        }
        
        # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        target_objects = []
        for keyword, objects in object_mapping.items():
            if keyword in instruction_lower:
                target_objects.extend(objects)
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã, –ø—Ä–æ–±—É–µ–º –æ–±—â–∏–π –∞–Ω–∞–ª–∏–∑
        if not target_objects:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            words = instruction_lower.split()
            for word in words:
                if len(word) > 4:  # –¢–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞
                    target_objects.append(word)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ—Å—Ç–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ
        return list(set(target_objects))
    
    def _calculate_clip_score(self, image, object_description):
        """–í—ã—á–∏—Å–ª—è–µ—Ç score —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –æ–±—ä–µ–∫—Ç–∞"""
        try:
            if self.model_type == "openclip":
                # OpenCLIP –ø–æ–¥—Ö–æ–¥
                image_tensor = self.processor(image).unsqueeze(0).to(self.device)
                text_tokens = self.tokenizer([f"a photo of {object_description}"]).to(self.device)
                
                with torch.no_grad():
                    image_features = self.model.encode_image(image_tensor)
                    text_features = self.model.encode_text(text_tokens)
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ –≤—ã—á–∏—Å–ª—è–µ–º similarity
                    image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                    text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                    
                    similarity = torch.mm(image_features, text_features.t())
                    score = similarity.item()
            
            else:
                # Transformers CLIP –ø–æ–¥—Ö–æ–¥
                inputs = self.processor(
                    text=[f"a photo of {object_description}"],
                    images=image,
                    return_tensors="pt",
                    padding=True
                ).to(self.device)
                
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    logits_per_image = outputs.logits_per_image
                    probs = logits_per_image.softmax(dim=1)
                    score = probs[0][0].item()
            
            return score
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ CLIP score –¥–ª—è '{object_description}': {e}")
            return 0.0
    
    def analyze_all_cells(self, cells_dir, instruction, threshold=0.3):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ —è—á–µ–π–∫–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–µ"""
        if not self.load_clip_model():
            return []
        
        cells_path = Path(cells_dir)
        if not cells_path.exists():
            print(f"‚ùå –ü–∞–ø–∫–∞ —Å —è—á–µ–π–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {cells_dir}")
            return []
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ñ–∞–π–ª—ã —è—á–µ–µ–∫
        cell_files = sorted(list(cells_path.glob("cell_*.png")))
        
        if not cell_files:
            print("‚ùå –§–∞–π–ª—ã —è—á–µ–µ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return []
        
        print(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(cell_files)} —è—á–µ–µ–∫ —Å –ø–æ–º–æ—â—å—é CLIP...")
        print(f"üìã –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: '{instruction}'")
        print(f"‚ö° –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {threshold}")
        
        matching_cells = []
        
        for cell_file in cell_files:
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä —è—á–µ–π–∫–∏ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                cell_index = int(cell_file.stem.replace("cell_", ""))
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —è—á–µ–π–∫–∏
                cell_image = Image.open(cell_file)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å –ø–æ–º–æ—â—å—é CLIP
                score, best_object = self.analyze_cell_with_clip(cell_image, instruction)
                
                print(f"   –Ø—á–µ–π–∫–∞ {cell_index}: {score:.3f} ({best_object})")
                
                # –ï—Å–ª–∏ score –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ - —è—á–µ–π–∫–∞ –ø–æ–¥—Ö–æ–¥–∏—Ç
                if score > threshold:
                    matching_cells.append({
                        "index": cell_index,
                        "score": score,
                        "object": best_object,
                        "file": str(cell_file)
                    })
                    print(f"   ‚úÖ –Ø—á–µ–π–∫–∞ {cell_index} –ø–æ–¥—Ö–æ–¥–∏—Ç: {best_object} ({score:.3f})")
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {cell_file}: {e}")
                continue
        
        print(f"\nüéØ –ù–∞–π–¥–µ–Ω–æ {len(matching_cells)} –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —è—á–µ–µ–∫")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é score
        matching_cells.sort(key=lambda x: x["score"], reverse=True)
        
        return matching_cells


def test_smart_detection(cells_dir="analysis/cells", instruction="–í—ã–±–µ—Ä–∏—Ç–µ —Ç–æ, –Ω–∞ —á—ë–º –º–æ–∂–Ω–æ —Å–∏–¥–µ—Ç—å"):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —É–º–Ω—É—é –¥–µ—Ç–µ–∫—Ü–∏—é"""
    detector = SmartDetector()
    
    results = detector.analyze_all_cells(cells_dir, instruction)
    
    print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê:")
    print("="*40)
    
    if results:
        for result in results:
            print(f"–Ø—á–µ–π–∫–∞ {result['index']}: {result['object']} (score: {result['score']:.3f})")
        
        print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: –ö–ª–∏–∫–Ω—É—Ç—å –Ω–∞ —è—á–µ–π–∫–∏ {[r['index'] for r in results]}")
    else:
        print("‚ùå –ü–æ–¥—Ö–æ–¥—è—â–∏—Ö —è—á–µ–µ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        print("üéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: –ù–∞–∂–∞—Ç—å '–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å'")
    
    return results


if __name__ == "__main__":
    print("üß† –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–º–Ω–æ–≥–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –æ–±—ä–µ–∫—Ç–æ–≤")
    print("="*50)
    
    test_smart_detection()
